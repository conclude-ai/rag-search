{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RAG Pipeline Optimization with RAGExperiment\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Setting up a document dataset and evaluation set\n",
    "2. Using `RAGExperiment` to test multiple RAG pipeline configurations\n",
    "3. Automatic evaluation with Precision@K, Recall@K, and MRR metrics\n",
    "4. Ranking and selecting the best pipeline configuration\n",
    "5. Integrating the optimized pipeline with LangChain for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '<your_openai_api_key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "# RAGExperiment for automated pipeline optimization\n",
    "from rag_select import RAGExperiment, ExperimentResults\n",
    "\n",
    "# Component implementations for search space\n",
    "from rag_select.parameter_impls.chunking_impls import (\n",
    "    SlidingWindowChunking,\n",
    "    LangChainRecursiveChunking,\n",
    ")\n",
    "from rag_select.parameter_impls.embedding_impls import (\n",
    "    HuggingFaceEmbedding,\n",
    ")\n",
    "from rag_select.parameter_impls.reranking_impls import (\n",
    "    CrossEncoderReranker,\n",
    ")\n",
    "\n",
    "# LangChain imports for Q&A (consume RAGArtifact as a retriever)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Setup: Create Sample Document Dataset\n",
    "\n",
    "We'll create a sample document corpus about various topics for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking variants: 4\n",
      "Embedding variants: 1\n",
      "Retriever variants: 3\n",
      "\n",
      "Total configurations: 12\n"
     ]
    }
   ],
   "source": [
    "# Define the search space - lists of component instances to test\n",
    "# RAGExperiment will run the Cartesian product of all combinations\n",
    "\n",
    "chunking_variants = [\n",
    "    SlidingWindowChunking(chunk_size=128, chunk_overlap=20),\n",
    "    SlidingWindowChunking(chunk_size=256, chunk_overlap=50),\n",
    "    SlidingWindowChunking(chunk_size=512, chunk_overlap=100),\n",
    "    LangChainRecursiveChunking(chunk_size=512, chunk_overlap=50),\n",
    "]\n",
    "\n",
    "embedding_variants = [\n",
    "    HuggingFaceEmbedding(model_name='sentence-transformers/all-MiniLM-L6-v2'),\n",
    "]\n",
    "\n",
    "# Retriever config: pass dicts with params (storage/embedding injected automatically)\n",
    "retriever_variants = [\n",
    "    {\"top_k\": 3},\n",
    "    {\"top_k\": 5},\n",
    "    {\"top_k\": 5, \"reranker\": CrossEncoderReranker(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')},\n",
    "]\n",
    "\n",
    "print(f\"Chunking variants: {len(chunking_variants)}\")\n",
    "print(f\"Embedding variants: {len(embedding_variants)}\")\n",
    "print(f\"Retriever variants: {len(retriever_variants)}\")\n",
    "print(f\"\\nTotal configurations: {len(chunking_variants) * len(embedding_variants) * len(retriever_variants)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99185da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 4\n",
      "Evaluation queries: 4\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Define corpus documents\n",
    "# -----------------------------\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "        \"doc_id\": \"doc_1\",\n",
    "        \"text\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation to improve factual accuracy.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_2\",\n",
    "        \"text\": \"Vector databases store embeddings and enable efficient similarity search for retrieval systems.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_3\",\n",
    "        \"text\": \"Chunking strategies such as sliding windows affect recall and precision in RAG pipelines.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"doc_4\",\n",
    "        \"text\": \"Embedding models map text into dense vector representations used for semantic search.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Define evaluation dataset\n",
    "# -----------------------------\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"query\": \"What is RAG?\",\n",
    "        \"relevant_doc_ids\": [\"doc_1\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What do vector databases do?\",\n",
    "        \"relevant_doc_ids\": [\"doc_2\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does chunking affect RAG systems?\",\n",
    "        \"relevant_doc_ids\": [\"doc_3\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are embeddings used for?\",\n",
    "        \"relevant_doc_ids\": [\"doc_4\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Documents: {len(documents)}\")\n",
    "print(f\"Evaluation queries: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Create Evaluation Set with Ground Truth\n",
    "\n",
    "Define queries and their relevant documents for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 12 experiments...\n",
      "\n",
      "[1/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.4167\n",
      "  precision@5: 0.2500\n",
      "  recall@5: 1.2500\n",
      "  mrr: 0.8750\n",
      "\n",
      "[2/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.4167\n",
      "  precision@5: 0.2500\n",
      "  recall@5: 1.2500\n",
      "  mrr: 0.8750\n",
      "\n",
      "[3/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.4167\n",
      "  precision@5: 0.2500\n",
      "  recall@5: 1.2500\n",
      "  mrr: 1.0000\n",
      "\n",
      "[4/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "[5/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "[6/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n",
      "[7/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "[8/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "[9/12] SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n",
      "[10/12] SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "[11/12] SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 0.8750\n",
      "\n",
      "[12/12] SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "  precision@3: 0.3333\n",
      "  precision@5: 0.2000\n",
      "  recall@5: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and run the experiment using RAGExperiment\n",
    "experiment = RAGExperiment(\n",
    "    dataset=eval_dataset,\n",
    "    documents=documents,\n",
    "    search_space={\n",
    "        \"chunking\": chunking_variants,\n",
    "        \"embedding\": embedding_variants,\n",
    "        \"retriever\": retriever_variants,\n",
    "    },\n",
    "    metrics=[\"precision@3\", \"precision@5\", \"recall@5\", \"mrr\"],\n",
    ")\n",
    "\n",
    "# Run all experiments (Cartesian product of all combinations)\n",
    "results = experiment.run(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Metrics\n",
    "\n",
    "Implement retrieval evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@3: 0.3333\n",
      "Recall@3: 1.0000\n",
      "MRR: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Metrics are automatically calculated by RAGExperiment:\n",
    "# - precision@k: Fraction of retrieved docs that are relevant (at cutoff k)\n",
    "# - recall@k: Fraction of relevant docs that were retrieved (at cutoff k)\n",
    "# - mrr: Mean Reciprocal Rank - measures how high the first relevant doc ranks\n",
    "\n",
    "# The MetricsCalculator class is available for manual use if needed:\n",
    "from rag_select.experiment import MetricsCalculator\n",
    "\n",
    "# Example usage:\n",
    "retrieved = [\"doc_1\", \"doc_3\", \"doc_2\"]\n",
    "relevant = [\"doc_1\"]\n",
    "\n",
    "print(f\"Precision@3: {MetricsCalculator.precision_at_k(retrieved, relevant, 3):.4f}\")\n",
    "print(f\"Recall@3: {MetricsCalculator.recall_at_k(retrieved, relevant, 3):.4f}\")\n",
    "print(f\"MRR: {MetricsCalculator.mrr(retrieved, relevant):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Define Pipeline Component Variations\n",
    "\n",
    "Create sets of component instances to test different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space:\n",
      "  Chunking: 4 variants\n",
      "  Embedding: 1 variants\n",
      "  Retriever: 3 variants\n",
      "\n",
      "Total configs tested: 12\n"
     ]
    }
   ],
   "source": [
    "# The search space was defined above with component instances:\n",
    "# - chunking_variants: Different chunking strategies\n",
    "# - embedding_variants: Different embedding models  \n",
    "# - retriever_variants: Different retrieval configurations (top_k, reranker)\n",
    "\n",
    "# RAGExperiment automatically:\n",
    "# - Uses SimpleIngestion and SimpleStorage as defaults\n",
    "# - Creates fresh storage instances per config (stateful)\n",
    "# - Injects storage/embedding into retrievers\n",
    "\n",
    "print(f\"Search space:\")\n",
    "print(f\"  Chunking: {len(chunking_variants)} variants\")\n",
    "print(f\"  Embedding: {len(embedding_variants)} variants\")\n",
    "print(f\"  Retriever: {len(retriever_variants)} variants\")\n",
    "print(f\"\\nTotal configs tested: {len(experiment.all_configs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Run Optimization Experiments\n",
    "\n",
    "Test each pipeline configuration and collect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results (ranked by mrr):\n",
      "================================================================================\n",
      "\n",
      "1. SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   Config ID: 3\n",
      "   mrr: 1.0000\n",
      "   precision@3: 0.4167\n",
      "   precision@5: 0.2500\n",
      "   recall@5: 1.2500\n",
      "\n",
      "2. SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   Config ID: 6\n",
      "   mrr: 1.0000\n",
      "   precision@3: 0.3333\n",
      "   precision@5: 0.2000\n",
      "   recall@5: 1.0000\n",
      "\n",
      "3. SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   Config ID: 9\n",
      "   mrr: 1.0000\n",
      "   precision@3: 0.3333\n",
      "   precision@5: 0.2000\n",
      "   recall@5: 1.0000\n",
      "\n",
      "4. SimpleIngestion | LangChainRecursiveChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   Config ID: 12\n",
      "   mrr: 1.0000\n",
      "   precision@3: 0.3333\n",
      "   precision@5: 0.2000\n",
      "   recall@5: 1.0000\n",
      "\n",
      "5. SimpleIngestion | SlidingWindowChunking | HuggingFaceEmbedding | SimpleStorage | SimpleRetriever\n",
      "   Config ID: 1\n",
      "   mrr: 0.8750\n",
      "   precision@3: 0.4167\n",
      "   precision@5: 0.2500\n",
      "   recall@5: 1.2500\n",
      "\n",
      "Best pipeline retrieved and ready for use!\n"
     ]
    }
   ],
   "source": [
    "# Rank results by MRR and get the best pipeline\n",
    "results.rank(by=\"mrr\")\n",
    "\n",
    "# Print summary of top results\n",
    "print(results.summary(top_k=5))\n",
    "\n",
    "# Get the best pipeline as a LangChain-compatible retriever\n",
    "best_artifact = results.get_best_pipeline()\n",
    "print(f\"\\nBest pipeline retrieved and ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Use Best Pipeline for Q&A\n",
    "\n",
    "Create a simple retrieval function using the best pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Retrieved Context:\n",
      "1. Retrieval-Augmented Generation (RAG) combines information retrieval with text generation to improve factual accuracy.\n",
      "2. Chunking strategies such as sliding windows affect recall and precision in RAG pipelines.\n",
      "3. accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval using the best artifact (RAGArtifact is a LangChain BaseRetriever)\n",
    "test_query = \"What is RAG?\"\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "docs = best_artifact.invoke(test_query)\n",
    "print(\"Retrieved Context:\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1og0sm2h7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is RAG?\n",
      "A: Retrieval-Augmented Generation (RAG) is a method that combines information retrieval with text generation to enhance the factual accuracy of generated content. It leverages external information sources to provide more reliable and contextually relevant responses.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What do vector databases do?\n",
      "A: Vector databases store embeddings and enable efficient similarity search for retrieval systems.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Use the provided context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create Q&A chain using the best artifact as retriever\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "qa_chain = create_retrieval_chain(best_artifact, doc_chain)\n",
    "\n",
    "# Test with sample questions from the evaluation dataset\n",
    "sample_questions = [\n",
    "    eval_dataset[0]['query'],\n",
    "    eval_dataset[1]['query'],\n",
    "]\n",
    "\n",
    "for q in sample_questions:\n",
    "    out = qa_chain.invoke({\"input\": q})\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", out[\"answer\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Save Best Configuration\n",
    "\n",
    "Save the best pipeline configuration for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best configuration info to best_rag_config.json\n",
      "Saved best artifact to best_artifact.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save best configuration info using pickle\n",
    "import pickle\n",
    "\n",
    "# Get the best result from ranked results\n",
    "best_result = list(results)[0]  # First item after ranking\n",
    "\n",
    "best_info = {\n",
    "    'config_name': best_result.config_name,\n",
    "    'metrics': best_result.metrics,\n",
    "    'component_params': best_result.component_params,\n",
    "}\n",
    "\n",
    "with open('best_rag_config.json', 'w') as f:\n",
    "    json.dump(best_info, f, indent=2)\n",
    "print(\"Saved best configuration info to best_rag_config.json\")\n",
    "\n",
    "with open('best_artifact.pkl', 'wb') as f:\n",
    "    pickle.dump(best_artifact, f)\n",
    "print(\"Saved best artifact to best_artifact.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Example: Creating a Custom Pipeline\n",
    "\n",
    "Demonstrate how to create a pipeline with specific component instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Results:\n",
      "  1. Retrieval-Augmented Generation (RAG) combines information retrieval with text ge...\n",
      "  2. Chunking strategies such as sliding windows affect recall and precision in RAG p...\n",
      "  3. Vector databases store embeddings and enable efficient similarity search for ret...\n",
      "  4. Embedding models map text into dense vector representations used for semantic se...\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a custom pipeline manually (without RAGExperiment)\n",
    "from rag_select import RAGClient\n",
    "from rag_select.parameter_impls.ingestion_impls import SimpleIngestion\n",
    "from rag_select.parameter_impls.storage_impls import SimpleStorage\n",
    "from rag_select.parameter_impls.retriever_impls import SimpleRetriever\n",
    "\n",
    "# Create custom components\n",
    "custom_chunking = SlidingWindowChunking(chunk_size=256, chunk_overlap=30)\n",
    "custom_embedding = HuggingFaceEmbedding(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "custom_storage = SimpleStorage()\n",
    "custom_retriever = SimpleRetriever(\n",
    "    storage=custom_storage,\n",
    "    embedding=custom_embedding,\n",
    "    top_k=5,\n",
    "    reranker=CrossEncoderReranker(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    ")\n",
    "\n",
    "# Build custom client\n",
    "custom_client = RAGClient(\n",
    "    ingestion=SimpleIngestion(),\n",
    "    chunking=custom_chunking,\n",
    "    embedding=custom_embedding,\n",
    "    storage=custom_storage,\n",
    "    retriever=custom_retriever,\n",
    ")\n",
    "\n",
    "# Ingest documents\n",
    "custom_client.upload_documents(documents)\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is RAG?\"\n",
    "results_manual = custom_client.retrieve(query)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Results:\")\n",
    "for i, result in enumerate(results_manual, 1):\n",
    "    print(f\"  {i}. {result['text'][:80]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
